name: Tests

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"
  pull_request:
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"

jobs:
  # ============================================================================
  # Node.js Tests
  # ============================================================================
  test-node:
    name: Node.js - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: NPM
            fixture: node-minimal
            package-manager: npm
            expected-node-enabled: "true"
            expected-package-manager: npm
            test-command: "node --version && npm --version"
            title: "üì¶ NPM Test Results"

          - name: PNPM
            fixture: node-pnpm
            expected-node-enabled: "true"
            expected-package-manager: pnpm
            test-command: "node --version && pnpm --version"
            title: "üì¶ PNPM Test Results"

          - name: Yarn
            fixture: node-yarn
            expected-node-enabled: "true"
            expected-package-manager: yarn
            test-command: "node --version && yarn --version"
            title: "üì¶ Yarn Test Results"

          - name: Cache Effectiveness
            fixture: cache-test
            test-cache: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: npm
            expected-package-manager-version: "10.2.4"
            title: "üíæ Cache Effectiveness Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          install-deps: "true"
          test-cache: ${{ matrix.test-cache || 'false' }}
          expected-node-version: ${{ matrix.expected-node-version || '' }}
          expected-node-enabled: ${{ matrix.expected-node-enabled || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}
          expected-package-manager-version: ${{ matrix.expected-package-manager-version || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"

              # Parse results JSON
              results = json.loads(results_str)

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "error": str(e),
                  "results": {}
              }
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-node-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("Test Results Output: '${{ toJson(steps.test.outputs.test-results) }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Bun Tests
  # ============================================================================
  test-bun:
    name: Bun - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: From devEngines
            fixture: bun-minimal
            expected-bun-enabled: "true"
            expected-package-manager: bun
            test-command: "bun --version && bun run index.ts"
            title: "üöÄ Bun from devEngines Test Results"

          - name: Explicit Version
            fixture: bun-minimal
            package-manager: bun
            package-manager-version: "1.3.3"
            bun-version: "1.3.3"
            expected-bun-enabled: "true"
            expected-bun-version: "1.3.3"
            test-command: "bun --version && bun run test.js"
            title: "üéØ Bun Explicit Version Test Results"

          - name: Lockfile
            fixture: bun-lockfile
            test-command: "bun --version"
            verify-lockfile: bun.lock
            title: "üîí Bun Lockfile Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          bun-version: ${{ matrix.bun-version || '' }}
          install-deps: "true"
          expected-bun-enabled: ${{ matrix.expected-bun-enabled || '' }}
          expected-bun-version: ${{ matrix.expected-bun-version || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify lockfile
        id: verify-lockfile
        if: matrix.verify-lockfile != ''
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          lockfile = "${{ matrix.verify-lockfile }}"
          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if not Path(lockfile).is_file():
              with open(github_output, "a") as f:
                  f.write("lockfile-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("lockfile-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              lockfile_verified = "${{ steps.verify-lockfile.outputs.lockfile-verified || 'n/a' }}"

              # Parse results JSON
              results = json.loads(results_str)

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "lockfile_verified": lockfile_verified,
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "error": str(e),
                  "results": {}
              }
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-bun-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("Test Results Output: '${{ toJson(steps.test.outputs.test-results) }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Deno Tests
  # ============================================================================
  test-deno:
    name: Deno - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: Explicit Input
            fixture: deno-minimal
            deno-version: "1.3.3"
            package-manager: deno
            package-manager-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-package-manager: deno
            expected-package-manager-version: "1.3.3"
            expected-deno-version: "1.3.3"
            test-command: "deno --version && deno run main.ts"
            title: "ü¶ï Deno Explicit Input Test Results"

          - name: From devEngines
            fixture: deno-lockfile
            expected-deno-enabled: "true"
            expected-package-manager: npm
            expected-deno-version: "2.5.6"
            test-command: "deno run app.ts"
            title: "üì¶ Deno from devEngines Test Results"

          - name: Explicit Version
            fixture: deno-minimal
            package-manager: deno
            package-manager-version: "1.3.3"
            deno-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-deno-version: "1.3.3"
            expected-package-manager: deno
            test-command: "deno --version && deno run test.ts"
            title: "üéØ Deno Explicit Version Test Results"

          - name: Lockfile
            fixture: deno-lockfile
            package-manager: deno
            package-manager-version: "2.5.6"
            deno-version: "2.5.6"
            test-command: "deno run main.ts"
            verify-lockfile: deno.lock
            expected-deno-enabled: "true"
            expected-deno-version: "2.5.6"
            expected-package-manager: deno
            title: "üîí Deno Lockfile Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          deno-version: ${{ matrix.deno-version || '' }}
          install-deps: "true"
          expected-deno-enabled: ${{ matrix.expected-deno-enabled || '' }}
          expected-deno-version: ${{ matrix.expected-deno-version || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify lockfile
        id: verify-lockfile
        if: matrix.verify-lockfile != ''
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          lockfile = "${{ matrix.verify-lockfile }}"
          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if not Path(lockfile).is_file():
              with open(github_output, "a") as f:
                  f.write("lockfile-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("lockfile-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              lockfile_verified = "${{ steps.verify-lockfile.outputs.lockfile-verified || 'n/a' }}"

              # Parse results JSON
              results = json.loads(results_str)

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "lockfile_verified": lockfile_verified,
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "error": str(e),
                  "results": {}
              }
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-deno-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("Test Results Output: '${{ toJson(steps.test.outputs.test-results) }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Feature Tests
  # ============================================================================
  test-features:
    name: Features - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: Biome from Config
            fixture: biome-auto
            expected-biome-enabled: "true"
            expected-biome-version: "2.3.6"
            test-command: "biome --version"
            title: "üß¨ Biome from Config Test Results"

          - name: Biome Explicit Version
            fixture: node-minimal
            biome-version: "2.3.6"
            expected-biome-enabled: "true"
            expected-biome-version: "2.3.6"
            test-command: "biome --version"
            title: "üéØ Biome Explicit Version Test Results"

          - name: Turbo Detection
            fixture: turbo-monorepo
            expected-turbo-enabled: "true"
            test-command: "echo 'Turbo detected'"
            title: "‚ö° Turbo Detection Test Results"

          - name: Skip Dependencies
            fixture: cache-test
            install-deps: "false"
            test-command: "node --version && npm --version"
            verify-no-node-modules: "true"
            title: "üö´ Skip Dependencies Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          biome-version: ${{ matrix.biome-version || '' }}
          install-deps: ${{ matrix.install-deps || 'true' }}
          expected-biome-enabled: ${{ matrix.expected-biome-enabled || '' }}
          expected-biome-version: ${{ matrix.expected-biome-version || '' }}
          expected-turbo-enabled: ${{ matrix.expected-turbo-enabled || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify no node_modules
        id: verify-no-modules
        if: matrix.verify-no-node-modules == 'true'
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if Path("node_modules").is_dir():
              with open(github_output, "a") as f:
                  f.write("no-modules-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("no-modules-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              no_modules = "${{ steps.verify-no-modules.outputs.no-modules-verified || 'n/a' }}"

              # Parse results JSON
              results = json.loads(results_str)

              # Check no_modules for overall pass/fail
              if no_modules != "n/a" and no_modules != "true":
                  passed = "false"

              # Add no_modules_verified to results if it was checked
              if no_modules != "n/a":
                  status = "passed" if no_modules == "true" else "failed"
                  results["no-node-modules"] = {
                      "actual": no_modules,
                      "expected": "true",
                      "status": status
                  }

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "error": str(e),
                  "results": {}
              }
              output_file = Path("test-results") / "${{ matrix.name }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-features-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("Test Results Output: '${{ toJson(steps.test.outputs.test-results) }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Test Summary
  # ============================================================================
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-node, test-bun, test-deno, test-features]
    if: always()
    steps:
      - name: Download all test results
        continue-on-error: true
        uses: actions/download-artifact@v5
        with:
          pattern: test-results-*
          path: all-results
          merge-multiple: true

      - name: Generate aggregated summary
        shell: python
        run: |
          import json
          import os
          import sys
          from pathlib import Path

          # Get summary file path
          summary_file = Path(os.environ["GITHUB_STEP_SUMMARY"])

          # Initialize counters
          total = 0
          passed = 0
          failed = 0
          skipped = 0
          parse_errors = []

          # Write header
          summary_file.write_text("## üß™ Test Results\n\n")

          # Create results directory if it doesn't exist
          results_dir = Path("all-results")
          results_dir.mkdir(exist_ok=True)

          # Check if there are any test results
          result_files = list(results_dir.glob("*.json"))
          if not result_files:
              with open(summary_file, "a") as f:
                  f.write("‚ö†Ô∏è No test results found - all test jobs may have failed before uploading results.\n")
              sys.exit(1)

          # Process all JSON result files
          for file_path in sorted(result_files):
              try:
                  # Try to parse JSON
                  data = json.loads(file_path.read_text())

                  total += 1

                  # Extract test info
                  name = data.get("name", "Unknown")
                  title = data.get("title", "Unknown")
                  fixture = data.get("fixture", "Unknown")
                  test_passed = data.get("passed", False)

                  # Count passes/fails
                  if test_passed:
                      passed += 1
                      status = "‚úÖ"
                  else:
                      failed += 1
                      status = "‚ùå"

                  # Build summary content starting with header
                  summary_content = f"### {status} {title}\n\n"

                  # Add expandable details
                  summary_content += "<details>\n"
                  if test_passed:
                      summary_content += f"<summary>{name}</summary>\n"
                  else:
                      summary_content += f"<summary>{name} (failed)</summary>\n"
                  summary_content += "\n"

                  # Show test metadata
                  summary_content += f"**Fixture:** `{fixture}`\n\n"

                  # Parse and display results
                  results = data.get("results", {})
                  if results:
                      summary_content += "**Results:**\n\n"
                      summary_content += "| Output | Actual | Expected | Status |\n"
                      summary_content += "|--------|--------|----------|--------|\n"

                      for key, value in results.items():
                          actual = value.get("actual", "")
                          expected = value.get("expected", "")
                          result_status = value.get("status", "")

                          # Format status with emoji
                          status_icons = {"passed": "‚úÖ", "failed": "‚ùå", "info": "‚ÑπÔ∏è"}
                          status_icon = status_icons.get(result_status, "‚Äî")

                          # Handle empty expected
                          expected_display = expected if expected else "‚Äî"

                          summary_content += f"| `{key}` | `{actual}` | `{expected_display}` | {status_icon} |\n"

                  # Show additional verification info if present
                  lockfile_verified = data.get("lockfile_verified", "n/a")
                  no_modules_verified = data.get("no_modules_verified", "n/a")
                  deps_verified = data.get("deps_verified", "n/a")
                  cache_verified = data.get("cache_verified", "n/a")

                  if any(v != "n/a" for v in [lockfile_verified, no_modules_verified, deps_verified, cache_verified]):
                      summary_content += "\n**Additional Verifications:**\n"
                      if lockfile_verified != "n/a":
                          summary_content += f"- Lockfile verified: {lockfile_verified}\n"
                      if no_modules_verified != "n/a":
                          summary_content += f"- No modules verified: {no_modules_verified}\n"
                      if deps_verified != "n/a":
                          summary_content += f"- Dependencies verified: {deps_verified}\n"
                      if cache_verified != "n/a":
                          summary_content += f"- Cache verified: {cache_verified}\n"

                  # Add raw JSON section
                  summary_content += "\n<details>\n"
                  summary_content += "<summary>Raw Test Data (JSON)</summary>\n\n"
                  summary_content += "```json\n"
                  summary_content += json.dumps(data, indent=2)
                  summary_content += "\n```\n"
                  summary_content += "</details>\n\n"
                  summary_content += "</details>\n\n"

                  with open(summary_file, "a") as f:
                      f.write(summary_content)

              except json.JSONDecodeError:
                  skipped += 1
                  parse_errors.append(file_path.name)
                  print(f"‚ö†Ô∏è Skipping malformed JSON: {file_path.name}", file=sys.stderr)

          # Add separator
          with open(summary_file, "a") as f:
              f.write("---\n\n")

          # Report parse errors if any
          if parse_errors:
              error_summary = "### ‚ö†Ô∏è Parse Errors\n\n"
              error_summary += "The following test result files could not be parsed (likely due to job failures):\n\n"
              for error_file in parse_errors:
                  error_summary += f"- `{error_file}`\n"
              error_summary += "\n---\n\n"
              with open(summary_file, "a") as f:
                  f.write(error_summary)

          # Final summary
          if total == 0 and skipped == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No test results found\n\n")
                  f.write("This usually means all test jobs failed before uploading results.\n")
              sys.exit(1)
          elif total == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No valid test results\n\n")
                  f.write(f"**Skipped:** {skipped} (malformed JSON)\n\n")
                  f.write("All test result files were malformed. Check individual job logs for details.\n")
              sys.exit(0)
          elif failed == 0 and skipped == 0:
              with open(summary_file, "a") as f:
                  f.write(f"### ‚úÖ All {total} tests passed!\n")
              sys.exit(0)
          elif failed == 0:
              with open(summary_file, "a") as f:
                  f.write(f"### ‚úÖ All {total} valid tests passed\n\n")
                  f.write(f"**Passed:** {passed} / **Total:** {total}\n")
                  if skipped > 0:
                      f.write(f"\n‚ö†Ô∏è **Skipped:** {skipped} test(s) with malformed JSON - check job logs for details\n")
              sys.exit(0)
          else:
              with open(summary_file, "a") as f:
                  f.write(f"### ‚ùå {failed} of {total} tests failed\n\n")
                  f.write(f"**Passed:** {passed} / **Failed:** {failed} / **Total:** {total}\n")
                  if skipped > 0:
                      f.write(f"\n‚ö†Ô∏è **Skipped:** {skipped} test(s) with malformed JSON - check job logs for details\n")
              sys.exit(0)
