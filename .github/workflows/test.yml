name: Tests

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"
  pull_request:
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"

concurrency:
  group: "tests-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  # ============================================================================
  # Node.js Tests - Create Cache
  # ============================================================================
  test-node-create-cache:
    name: node | ${{ matrix.pm }} | create cache | ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        pm: [npm, pnpm, yarn, multi]
        include:
          # NPM - create cache
          - pm: npm
            fixture: node-npm
            install-deps: "true"
            expected-cache-hit: "false"
            expected-node-enabled: "true"
            expected-node-version: "24.9.0"
            expected-package-manager: npm
            expected-package-manager-version: "11.6.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && npm --version"
            title: "üì¶ NPM (Create Cache)"

          # PNPM - create cache
          - pm: pnpm
            fixture: node-pnpm
            install-deps: "true"
            expected-cache-hit: "false"
            expected-node-enabled: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: pnpm
            expected-package-manager-version: "10.20.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && pnpm --version"
            title: "üì¶ PNPM (Create Cache)"

          # Yarn - create cache
          - pm: yarn
            fixture: node-yarn
            install-deps: "true"
            expected-cache-hit: "false"
            expected-node-enabled: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: yarn
            expected-package-manager-version: "4.0.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && yarn --version"
            title: "üì¶ Yarn (Create Cache)"

          # Multi-runtime - create cache
          - pm: multi
            fixture: node-multi
            install-deps: "true"
            expected-cache-hit: "false"
            expected-node-enabled: "true"
            expected-node-version: "25.0.0"
            expected-bun-enabled: "true"
            expected-bun-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-deno-version: "2.5.6"
            expected-package-manager: pnpm
            expected-package-manager-version: "10.23.0"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && bun --version && deno --version && pnpm --version"
            title: "üì¶ Multi-Runtime (Create Cache)"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          install-deps: ${{ matrix.install-deps }}
          cache-hash: ${{ matrix.pm }}-${{ matrix.os }}-${{ github.run_id }}
          expected-node-version: ${{ matrix.expected-node-version || '' }}
          expected-node-enabled: ${{ matrix.expected-node-enabled || '' }}
          expected-bun-enabled: ${{ matrix.expected-bun-enabled || '' }}
          expected-deno-enabled: ${{ matrix.expected-deno-enabled || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}
          expected-package-manager-version: ${{ matrix.expected-package-manager-version || '' }}
          expected-biome-enabled: ${{ matrix.expected-biome-enabled || '' }}
          expected-cache-hit: ${{ matrix.expected-cache-hit || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          import traceback
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ steps.test.outputs.test-results }}''' or "{}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Extract error from results if present
              error = results.get("error", "") if isinstance(results, dict) else ""

              # Build final JSON
              test_name = "${{ matrix.pm }}-create-cache"
              output = {
                  "name": f"{test_name}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": test_name,
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true" if not error else False,
                  "results": results if not error else {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": error
              }

              # Write to file (slugify name to avoid spaces in filenames)
              Path("test-results").mkdir(exist_ok=True)
              safe_name = test_name.replace(" ", "-").lower()
              output_file = Path("test-results") / f"{safe_name}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")
              traceback.print_exc()

              Path("test-results").mkdir(exist_ok=True)
              test_name = "${{ matrix.pm }}-create-cache"
              error_output = {
                  "name": f"{test_name}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": test_name,
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              safe_name = test_name.replace(" ", "-").lower()
              output_file = Path("test-results") / f"{safe_name}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-node-create-cache-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Node.js Tests - Restore Cache
  # ============================================================================
  test-node-restore-cache:
    name: node | ${{ matrix.pm }} | restore cache | ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [test-node-create-cache]
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        pm: [npm, pnpm, yarn, multi]
        include:
          # NPM - restore cache
          - pm: npm
            fixture: node-npm
            install-deps: "true"
            expected-cache-hit: "true"
            expected-node-enabled: "true"
            expected-node-version: "24.9.0"
            expected-package-manager: npm
            expected-package-manager-version: "11.6.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && npm --version"
            title: "üì¶ NPM (Restore Cache)"

          # PNPM - restore cache
          - pm: pnpm
            fixture: node-pnpm
            install-deps: "true"
            expected-cache-hit: "true"
            expected-node-enabled: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: pnpm
            expected-package-manager-version: "10.20.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && pnpm --version"
            title: "üì¶ PNPM (Restore Cache)"

          # Yarn - restore cache
          - pm: yarn
            fixture: node-yarn
            install-deps: "true"
            expected-cache-hit: "true"
            expected-node-enabled: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: yarn
            expected-package-manager-version: "4.0.0"
            expected-deno-enabled: "false"
            expected-bun-enabled: "false"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && yarn --version"
            title: "üì¶ Yarn (Restore Cache)"

          # Multi-runtime - restore cache
          - pm: multi
            fixture: node-multi
            install-deps: "true"
            expected-cache-hit: "true"
            expected-node-enabled: "true"
            expected-node-version: "25.0.0"
            expected-bun-enabled: "true"
            expected-bun-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-deno-version: "2.5.6"
            expected-package-manager: pnpm
            expected-package-manager-version: "10.23.0"
            expected-biome-enabled: "false"
            expected-turbo-enabled: "false"
            test-command: "node --version && bun --version && deno --version && pnpm --version"
            title: "üì¶ Multi-Runtime (Restore Cache)"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          install-deps: ${{ matrix.install-deps }}
          cache-hash: ${{ matrix.pm }}-${{ matrix.os }}-${{ github.run_id }}
          expected-node-version: ${{ matrix.expected-node-version || '' }}
          expected-node-enabled: ${{ matrix.expected-node-enabled || '' }}
          expected-bun-enabled: ${{ matrix.expected-bun-enabled || '' }}
          expected-deno-enabled: ${{ matrix.expected-deno-enabled || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}
          expected-package-manager-version: ${{ matrix.expected-package-manager-version || '' }}
          expected-biome-enabled: ${{ matrix.expected-biome-enabled || '' }}
          expected-cache-hit: ${{ matrix.expected-cache-hit || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          import traceback
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ steps.test.outputs.test-results }}''' or "{}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Extract error from results if present
              error = results.get("error", "") if isinstance(results, dict) else ""

              # Build final JSON
              test_name = "${{ matrix.pm }}-restore-cache"
              output = {
                  "name": f"{test_name}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": test_name,
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true" if not error else False,
                  "results": results if not error else {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": error
              }

              # Write to file (slugify name to avoid spaces in filenames)
              Path("test-results").mkdir(exist_ok=True)
              safe_name = test_name.replace(" ", "-").lower()
              output_file = Path("test-results") / f"{safe_name}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")
              traceback.print_exc()

              Path("test-results").mkdir(exist_ok=True)
              test_name = "${{ matrix.pm }}-restore-cache"
              error_output = {
                  "name": f"{test_name}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": test_name,
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              safe_name = test_name.replace(" ", "-").lower()
              output_file = Path("test-results") / f"{safe_name}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-node-restore-cache-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Test Summary
  # ============================================================================
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-node-create-cache, test-node-restore-cache]
    if: always()
    steps:
      - name: Download all test results
        continue-on-error: true
        uses: actions/download-artifact@v5
        with:
          pattern: test-results-*
          path: all-results
          merge-multiple: true

      - name: Generate aggregated summary
        shell: python
        run: |
          import json
          import os
          import sys
          from pathlib import Path
          from collections import defaultdict

          # Get summary file path
          summary_file = Path(os.environ["GITHUB_STEP_SUMMARY"])

          # Write header
          summary_file.write_text("## üß™ Test Results\n\n")

          # Create results directory if it doesn't exist
          results_dir = Path("all-results")
          results_dir.mkdir(exist_ok=True)

          # Check if there are any test results
          result_files = list(results_dir.glob("*.json"))
          if not result_files:
              with open(summary_file, "a") as f:
                  f.write("‚ö†Ô∏è No test results found - all test jobs may have failed before uploading results.\n")
              sys.exit(1)

          # Group results by test title
          tests_by_title = defaultdict(list)
          parse_errors = []

          # Parse and group all test results
          for file_path in sorted(result_files):
              try:
                  data = json.loads(file_path.read_text())
                  title = data.get("title", "Unknown")
                  tests_by_title[title].append(data)
              except json.JSONDecodeError:
                  parse_errors.append(file_path.name)
                  print(f"‚ö†Ô∏è Skipping malformed JSON: {file_path.name}", file=sys.stderr)

          # Track overall stats
          total_tests = 0
          total_passed = 0
          total_failed = 0

          # Generate summary for each test type
          for title in sorted(tests_by_title.keys()):
              os_results = tests_by_title[title]

              # Determine overall status for this test type
              all_passed = all(r.get("passed", False) for r in os_results)
              any_failed = any(not r.get("passed", False) for r in os_results)
              overall_status = "‚úÖ" if all_passed else "‚ùå"

              total_tests += 1
              if all_passed:
                  total_passed += 1
              else:
                  total_failed += 1

              # Write test header
              summary_content = f"### {overall_status} {title}\n\n"

              # Add each OS as its own details section
              for os_data in sorted(os_results, key=lambda x: x.get("os", "")):
                  os_name = os_data.get("os", "Unknown")
                  fixture = os_data.get("fixture", "Unknown")
                  passed = os_data.get("passed", False)
                  results = os_data.get("results", {})
                  os_status = "‚úÖ" if passed else "‚ùå"

                  # Auto-expand if this OS failed
                  os_open = " open" if not passed else ""
                  summary_content += f"<details{os_open}>\n"
                  summary_content += f"<summary>{os_status} <code>{os_name}</code></summary>\n\n"
                  summary_content += f"**Fixture:** `{fixture}`\n\n"

                  # Show error if exists
                  error = os_data.get("error", "")
                  if error:
                      summary_content += f"**‚ö†Ô∏è Error:**\n```\n{error}\n```\n\n"

                  # Show test results table
                  if results:
                      summary_content += "**Results:**\n\n"
                      summary_content += "| Output | Actual | Expected | Status |\n"
                      summary_content += "|--------|--------|----------|--------|\n"

                      for key, value in results.items():
                          actual = value.get("actual", "")
                          expected = value.get("expected", "")
                          result_status = value.get("status", "")
                          status_icons = {"passed": "‚úÖ", "failed": "‚ùå", "info": "‚ÑπÔ∏è"}
                          status_icon = status_icons.get(result_status, "‚Äî")
                          expected_display = expected if expected else "‚Äî"
                          summary_content += f"| `{key}` | `{actual}` | `{expected_display}` | {status_icon} |\n"

                  # Show additional verifications
                  lockfile_verified = os_data.get("lockfile_verified", "n/a")
                  no_modules_verified = os_data.get("no_modules_verified", "n/a")
                  deps_verified = os_data.get("deps_verified", "n/a")
                  cache_verified = os_data.get("cache_verified", "n/a")

                  if any(v != "n/a" for v in [lockfile_verified, no_modules_verified, deps_verified, cache_verified]):
                      summary_content += "\n**Additional Verifications:**\n"
                      if lockfile_verified != "n/a":
                          summary_content += f"- Lockfile verified: {lockfile_verified}\n"
                      if no_modules_verified != "n/a":
                          summary_content += f"- No modules verified: {no_modules_verified}\n"
                      if deps_verified != "n/a":
                          summary_content += f"- Dependencies verified: {deps_verified}\n"
                      if cache_verified != "n/a":
                          summary_content += f"- Cache verified: {cache_verified}\n"

                  # Show raw JSON data
                  summary_content += "\n<details>\n"
                  summary_content += "<summary>üìÑ Raw JSON Data</summary>\n\n"
                  summary_content += "```json\n"
                  summary_content += json.dumps(os_data, indent=2)
                  summary_content += "\n```\n\n"
                  summary_content += "</details>\n"

                  summary_content += "\n</details>\n\n"

              with open(summary_file, "a") as f:
                  f.write(summary_content)

          # Add separator
          with open(summary_file, "a") as f:
              f.write("---\n\n")

          # Report parse errors if any
          if parse_errors:
              error_summary = "### ‚ö†Ô∏è Parse Errors\n\n"
              error_summary += "The following test result files could not be parsed (likely due to job failures):\n\n"
              for error_file in parse_errors:
                  error_summary += f"- `{error_file}`\n"
              error_summary += "\n"
              with open(summary_file, "a") as f:
                  f.write(error_summary)

          # Final summary
          if total_tests == 0 and len(parse_errors) == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No test results found\n\n")
                  f.write("This usually means all test jobs failed before uploading results.\n")
              sys.exit(1)
          elif total_tests == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No valid test results\n\n")
                  f.write(f"**Parse Errors:** {len(parse_errors)}\n\n")
                  f.write("All test result files were malformed. Check individual job logs for details.\n")
              sys.exit(1)
          elif total_failed == 0:
              with open(summary_file, "a") as f:
                  if len(parse_errors) == 0:
                      f.write(f"### ‚úÖ All {total_tests} test types passed across all platforms!\n")
                  else:
                      f.write(f"### ‚úÖ All {total_tests} valid test types passed\n\n")
                      f.write(f"‚ö†Ô∏è **Note:** {len(parse_errors)} test(s) had parse errors - check above for details\n")
              sys.exit(0)
          else:
              with open(summary_file, "a") as f:
                  f.write(f"### ‚ùå {total_failed} of {total_tests} test types failed\n\n")
                  f.write(f"**Passed:** {total_passed} / **Failed:** {total_failed} / **Total:** {total_tests}\n")
                  if len(parse_errors) > 0:
                      f.write(f"\n‚ö†Ô∏è **Parse Errors:** {len(parse_errors)} test(s) - check above for details\n")
              sys.exit(1)
