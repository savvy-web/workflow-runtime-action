name: Tests

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"
  pull_request:
    paths:
      - "src/**"
      - "dist/**"
      - "action.yml"
      - "__fixtures__/**"
      - ".github/actions/runtime/**"
      - ".github/actions/test-fixture/**"
      - ".github/workflows/test.yml"

jobs:
  # ============================================================================
  # Node.js Tests
  # ============================================================================
  test-node:
    name: Node.js - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: NPM
            fixture: node-minimal
            package-manager: npm
            expected-node-enabled: "true"
            expected-package-manager: npm
            test-command: "node --version && npm --version"
            title: "üì¶ NPM Test Results"

          - name: PNPM
            fixture: node-pnpm
            expected-node-enabled: "true"
            expected-package-manager: pnpm
            test-command: "node --version && pnpm --version"
            title: "üì¶ PNPM Test Results"

          - name: Yarn
            fixture: node-yarn
            expected-node-enabled: "true"
            expected-package-manager: yarn
            test-command: "node --version && yarn --version"
            title: "üì¶ Yarn Test Results"

          - name: Cache Effectiveness
            fixture: cache-test
            test-cache: "true"
            expected-node-version: "20.11.0"
            expected-package-manager: npm
            expected-package-manager-version: "10.2.4"
            title: "üíæ Cache Effectiveness Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          install-deps: "true"
          test-cache: ${{ matrix.test-cache || 'false' }}
          expected-node-version: ${{ matrix.expected-node-version || '' }}
          expected-node-enabled: ${{ matrix.expected-node-enabled || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}
          expected-package-manager-version: ${{ matrix.expected-package-manager-version || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-node-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Bun Tests
  # ============================================================================
  test-bun:
    name: Bun - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: From devEngines
            fixture: bun-minimal
            expected-bun-enabled: "true"
            expected-package-manager: bun
            test-command: "bun --version && bun run index.ts"
            title: "üöÄ Bun from devEngines Test Results"

          - name: Explicit Version
            fixture: bun-minimal
            package-manager: bun
            package-manager-version: "1.3.3"
            bun-version: "1.3.3"
            expected-bun-enabled: "true"
            expected-bun-version: "1.3.3"
            test-command: "bun --version && bun run test.js"
            title: "üéØ Bun Explicit Version Test Results"

          - name: Lockfile
            fixture: bun-lockfile
            test-command: "bun --version"
            verify-lockfile: bun.lock
            title: "üîí Bun Lockfile Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          bun-version: ${{ matrix.bun-version || '' }}
          install-deps: "true"
          expected-bun-enabled: ${{ matrix.expected-bun-enabled || '' }}
          expected-bun-version: ${{ matrix.expected-bun-version || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify lockfile
        id: verify-lockfile
        if: matrix.verify-lockfile != ''
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          lockfile = "${{ matrix.verify-lockfile }}"
          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if not Path(lockfile).is_file():
              with open(github_output, "a") as f:
                  f.write("lockfile-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("lockfile-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              lockfile_verified = "${{ steps.verify-lockfile.outputs.lockfile-verified || 'n/a' }}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "lockfile_verified": lockfile_verified,
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-bun-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Deno Tests
  # ============================================================================
  test-deno:
    name: Deno - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: Explicit Input
            fixture: deno-minimal
            deno-version: "1.3.3"
            package-manager: deno
            package-manager-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-package-manager: deno
            expected-package-manager-version: "1.3.3"
            expected-deno-version: "1.3.3"
            test-command: "deno --version && deno run main.ts"
            title: "ü¶ï Deno Explicit Input Test Results"

          - name: From devEngines
            fixture: deno-lockfile
            expected-deno-enabled: "true"
            expected-package-manager: npm
            expected-deno-version: "2.5.6"
            test-command: "deno run app.ts"
            title: "üì¶ Deno from devEngines Test Results"

          - name: Explicit Version
            fixture: deno-minimal
            package-manager: deno
            package-manager-version: "1.3.3"
            deno-version: "1.3.3"
            expected-deno-enabled: "true"
            expected-deno-version: "1.3.3"
            expected-package-manager: deno
            test-command: "deno --version && deno run test.ts"
            title: "üéØ Deno Explicit Version Test Results"

          - name: Lockfile
            fixture: deno-lockfile
            package-manager: deno
            package-manager-version: "2.5.6"
            deno-version: "2.5.6"
            test-command: "deno run main.ts"
            verify-lockfile: deno.lock
            expected-deno-enabled: "true"
            expected-deno-version: "2.5.6"
            expected-package-manager: deno
            title: "üîí Deno Lockfile Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          package-manager: ${{ matrix.package-manager || '' }}
          deno-version: ${{ matrix.deno-version || '' }}
          install-deps: "true"
          expected-deno-enabled: ${{ matrix.expected-deno-enabled || '' }}
          expected-deno-version: ${{ matrix.expected-deno-version || '' }}
          expected-package-manager: ${{ matrix.expected-package-manager || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify lockfile
        id: verify-lockfile
        if: matrix.verify-lockfile != ''
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          lockfile = "${{ matrix.verify-lockfile }}"
          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if not Path(lockfile).is_file():
              with open(github_output, "a") as f:
                  f.write("lockfile-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("lockfile-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              lockfile_verified = "${{ steps.verify-lockfile.outputs.lockfile-verified || 'n/a' }}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "lockfile_verified": lockfile_verified,
                  "results": results
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-deno-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Feature Tests
  # ============================================================================
  test-features:
    name: Features - ${{ matrix.name }} (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - name: Biome from Config
            fixture: biome-auto
            expected-biome-enabled: "true"
            expected-biome-version: "2.3.6"
            test-command: "biome --version"
            title: "üß¨ Biome from Config Test Results"

          - name: Biome Explicit Version
            fixture: node-minimal
            biome-version: "2.3.6"
            expected-biome-enabled: "true"
            expected-biome-version: "2.3.6"
            test-command: "biome --version"
            title: "üéØ Biome Explicit Version Test Results"

          - name: Turbo Detection
            fixture: turbo-monorepo
            expected-turbo-enabled: "true"
            test-command: "echo 'Turbo detected'"
            title: "‚ö° Turbo Detection Test Results"

          - name: Skip Dependencies
            fixture: cache-test
            install-deps: "false"
            test-command: "node --version && npm --version"
            verify-no-node-modules: "true"
            title: "üö´ Skip Dependencies Test Results"

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Test fixture
        id: test
        continue-on-error: true
        uses: ./.github/actions/test-fixture
        with:
          fixture: ${{ matrix.fixture }}
          title: ${{ matrix.title }}
          biome-version: ${{ matrix.biome-version || '' }}
          install-deps: ${{ matrix.install-deps || 'true' }}
          expected-biome-enabled: ${{ matrix.expected-biome-enabled || '' }}
          expected-biome-version: ${{ matrix.expected-biome-version || '' }}
          expected-turbo-enabled: ${{ matrix.expected-turbo-enabled || '' }}

      - name: Test runtime
        if: matrix.test-command != ''
        continue-on-error: true
        run: ${{ matrix.test-command }}

      - name: Verify no node_modules
        id: verify-no-modules
        if: matrix.verify-no-node-modules == 'true'
        continue-on-error: true
        shell: python
        run: |
          import os
          import sys
          from pathlib import Path

          github_output = Path(os.environ["GITHUB_OUTPUT"])

          if Path("node_modules").is_dir():
              with open(github_output, "a") as f:
                  f.write("no-modules-verified=false\n")
              sys.exit(1)

          with open(github_output, "a") as f:
              f.write("no-modules-verified=true\n")

      - name: Save test results
        if: always()
        shell: python
        run: |
          import json
          import os
          from pathlib import Path

          try:
              # Get outputs from GitHub Actions
              passed = "${{ steps.test.outputs.test-passed }}" or "false"
              results_str = r'''${{ toJson(steps.test.outputs.test-results) }}''' or "{}"
              lockfile_verified = "${{ steps.verify-lockfile.outputs.lockfile-verified || 'n/a' }}"
              no_modules = "${{ steps.verify-no-modules.outputs.no-modules-verified || 'n/a' }}"

              # Parse results JSON - ensure it's always a dict
              results = json.loads(results_str)
              if not isinstance(results, dict):
                  results = {}

              # Check verifications for overall pass/fail
              if no_modules != "n/a" and no_modules != "true":
                  passed = "false"
              if lockfile_verified != "n/a" and lockfile_verified != "true":
                  passed = "false"

              # Add no_modules_verified to results if it was checked
              if no_modules != "n/a":
                  status = "passed" if no_modules == "true" else "failed"
                  results["no-node-modules"] = {
                      "actual": no_modules,
                      "expected": "true",
                      "status": status
                  }

              # Build final JSON
              output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": passed == "true",
                  "results": results,
                  "lockfile_verified": lockfile_verified,
                  "no_modules_verified": no_modules,
                  "error": ""
              }

              # Write to file
              Path("test-results").mkdir(exist_ok=True)
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(output, indent=2))

          except Exception as e:
              # Error handling - create error JSON
              print(f"::error::Failed to save test results: {e}")

              Path("test-results").mkdir(exist_ok=True)
              error_output = {
                  "name": "${{ matrix.name }}-${{ matrix.os }}",
                  "os": "${{ matrix.os }}",
                  "test_name": "${{ matrix.name }}",
                  "title": "${{ matrix.title }}",
                  "fixture": "${{ matrix.fixture }}",
                  "passed": False,
                  "results": {},
                  "lockfile_verified": "n/a",
                  "no_modules_verified": "n/a",
                  "error": str(e)
              }
              output_file = Path("test-results") / f"${{ matrix.name }}-${{ matrix.os }}.json"
              output_file.write_text(json.dumps(error_output, indent=2))

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-features-${{ matrix.os }}-${{ strategy.job-index }}
          path: test-results/
          retention-days: 1

      - name: Fail job if test failed
        if: always() && (steps.test.outputs.test-passed == 'false' || steps.test.outputs.test-passed == '')
        shell: python
        run: |
          import sys

          print("::group::Test Failure Details")
          print("Test Step Outcome: ${{ steps.test.outcome }}")
          print("Test Step Conclusion: ${{ steps.test.conclusion }}")
          print("Test Passed Output: '${{ steps.test.outputs.test-passed }}'")
          print("::endgroup::")

          # Determine failure reason
          outcome = "${{ steps.test.outcome }}"
          test_passed = "${{ steps.test.outputs.test-passed }}"

          if outcome == "failure":
              print("::error::Test fixture step failed - check logs above for details")
          elif test_passed == "false":
              print("::error::Tests ran but failed - check test results above")
          elif not test_passed:
              print("::error::Test fixture did not complete - no test-passed output set")
          else:
              print("::error::Unknown test failure - check logs above")

          sys.exit(1)

  # ============================================================================
  # Test Summary
  # ============================================================================
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-node, test-bun, test-deno, test-features]
    if: always()
    steps:
      - name: Download all test results
        continue-on-error: true
        uses: actions/download-artifact@v5
        with:
          pattern: test-results-*
          path: all-results
          merge-multiple: true

      - name: Generate aggregated summary
        shell: python
        run: |
          import json
          import os
          import sys
          from pathlib import Path
          from collections import defaultdict

          # Get summary file path
          summary_file = Path(os.environ["GITHUB_STEP_SUMMARY"])

          # Write header
          summary_file.write_text("## üß™ Test Results\n\n")

          # Create results directory if it doesn't exist
          results_dir = Path("all-results")
          results_dir.mkdir(exist_ok=True)

          # Check if there are any test results
          result_files = list(results_dir.glob("*.json"))
          if not result_files:
              with open(summary_file, "a") as f:
                  f.write("‚ö†Ô∏è No test results found - all test jobs may have failed before uploading results.\n")
              sys.exit(1)

          # Group results by test title
          tests_by_title = defaultdict(list)
          parse_errors = []

          # Parse and group all test results
          for file_path in sorted(result_files):
              try:
                  data = json.loads(file_path.read_text())
                  title = data.get("title", "Unknown")
                  tests_by_title[title].append(data)
              except json.JSONDecodeError:
                  parse_errors.append(file_path.name)
                  print(f"‚ö†Ô∏è Skipping malformed JSON: {file_path.name}", file=sys.stderr)

          # Track overall stats
          total_tests = 0
          total_passed = 0
          total_failed = 0

          # Generate summary for each test type
          for title in sorted(tests_by_title.keys()):
              os_results = tests_by_title[title]

              # Determine overall status for this test type
              all_passed = all(r.get("passed", False) for r in os_results)
              any_failed = any(not r.get("passed", False) for r in os_results)
              overall_status = "‚úÖ" if all_passed else "‚ùå"

              total_tests += 1
              if all_passed:
                  total_passed += 1
              else:
                  total_failed += 1

              # Write test header
              summary_content = f"### {overall_status} {title}\n\n"

              # List OS results
              for os_data in sorted(os_results, key=lambda x: x.get("os", "")):
                  os_name = os_data.get("os", "Unknown")
                  passed = os_data.get("passed", False)
                  os_status = "‚úÖ" if passed else "‚ùå"
                  summary_content += f"  {os_status} `{os_name}`\n"

              summary_content += "\n"

              # Add expandable details for each OS
              for os_data in sorted(os_results, key=lambda x: x.get("os", "")):
                  os_name = os_data.get("os", "Unknown")
                  fixture = os_data.get("fixture", "Unknown")
                  passed = os_data.get("passed", False)
                  results = os_data.get("results", {})

                  summary_content += f"<details>\n"
                  summary_content += f"<summary><code>{os_name}</code> - Details</summary>\n\n"
                  summary_content += f"**Fixture:** `{fixture}`\n\n"

                  # Show error if exists
                  error = os_data.get("error", "")
                  if error:
                      summary_content += f"**‚ö†Ô∏è Error:**\n```\n{error}\n```\n\n"

                  # Show test results table
                  if results:
                      summary_content += "**Results:**\n\n"
                      summary_content += "| Output | Actual | Expected | Status |\n"
                      summary_content += "|--------|--------|----------|--------|\n"

                      for key, value in results.items():
                          actual = value.get("actual", "")
                          expected = value.get("expected", "")
                          result_status = value.get("status", "")
                          status_icons = {"passed": "‚úÖ", "failed": "‚ùå", "info": "‚ÑπÔ∏è"}
                          status_icon = status_icons.get(result_status, "‚Äî")
                          expected_display = expected if expected else "‚Äî"
                          summary_content += f"| `{key}` | `{actual}` | `{expected_display}` | {status_icon} |\n"

                  # Show additional verifications
                  lockfile_verified = os_data.get("lockfile_verified", "n/a")
                  no_modules_verified = os_data.get("no_modules_verified", "n/a")
                  deps_verified = os_data.get("deps_verified", "n/a")
                  cache_verified = os_data.get("cache_verified", "n/a")

                  if any(v != "n/a" for v in [lockfile_verified, no_modules_verified, deps_verified, cache_verified]):
                      summary_content += "\n**Additional Verifications:**\n"
                      if lockfile_verified != "n/a":
                          summary_content += f"- Lockfile verified: {lockfile_verified}\n"
                      if no_modules_verified != "n/a":
                          summary_content += f"- No modules verified: {no_modules_verified}\n"
                      if deps_verified != "n/a":
                          summary_content += f"- Dependencies verified: {deps_verified}\n"
                      if cache_verified != "n/a":
                          summary_content += f"- Cache verified: {cache_verified}\n"

                  summary_content += "\n</details>\n\n"

              with open(summary_file, "a") as f:
                  f.write(summary_content)

          # Add separator
          with open(summary_file, "a") as f:
              f.write("---\n\n")

          # Report parse errors if any
          if parse_errors:
              error_summary = "### ‚ö†Ô∏è Parse Errors\n\n"
              error_summary += "The following test result files could not be parsed (likely due to job failures):\n\n"
              for error_file in parse_errors:
                  error_summary += f"- `{error_file}`\n"
              error_summary += "\n"
              with open(summary_file, "a") as f:
                  f.write(error_summary)

          # Final summary
          if total_tests == 0 and len(parse_errors) == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No test results found\n\n")
                  f.write("This usually means all test jobs failed before uploading results.\n")
              sys.exit(1)
          elif total_tests == 0:
              with open(summary_file, "a") as f:
                  f.write("### ‚ö†Ô∏è No valid test results\n\n")
                  f.write(f"**Parse Errors:** {len(parse_errors)}\n\n")
                  f.write("All test result files were malformed. Check individual job logs for details.\n")
              sys.exit(1)
          elif total_failed == 0:
              with open(summary_file, "a") as f:
                  if len(parse_errors) == 0:
                      f.write(f"### ‚úÖ All {total_tests} test types passed across all platforms!\n")
                  else:
                      f.write(f"### ‚úÖ All {total_tests} valid test types passed\n\n")
                      f.write(f"‚ö†Ô∏è **Note:** {len(parse_errors)} test(s) had parse errors - check above for details\n")
              sys.exit(0)
          else:
              with open(summary_file, "a") as f:
                  f.write(f"### ‚ùå {total_failed} of {total_tests} test types failed\n\n")
                  f.write(f"**Passed:** {total_passed} / **Failed:** {total_failed} / **Total:** {total_tests}\n")
                  if len(parse_errors) > 0:
                      f.write(f"\n‚ö†Ô∏è **Parse Errors:** {len(parse_errors)} test(s) - check above for details\n")
              sys.exit(1)
